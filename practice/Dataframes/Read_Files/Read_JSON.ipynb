{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06471431-a761-4742-af23-c43396987387",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Read JSON\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8792626b-6607-46c5-a58b-6c40f28149f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+----------+---------------+--------+-------+\n|Department|EmpID|FirstName|  HireDate|       JobTitle|LastName| Salary|\n+----------+-----+---------+----------+---------------+--------+-------+\n|   Finance|   38|  First38|2015-02-08|        Analyst|  Last38|59000.0|\n|     Sales|   39|  First39|2015-02-09|Sales Executive|  Last39|59500.0|\n|        HR|   40|  First40|2015-02-10|      Recruiter|  Last40|60000.0|\n|        IT|   41|  First41|2015-02-11|      Developer|  Last41|60500.0|\n|   Finance|   42|  First42|2015-02-12|        Analyst|  Last42|61000.0|\n|     Sales|   43|  First43|2015-02-13|Sales Executive|  Last43|61500.0|\n|        HR|   44|  First44|2015-02-14|      Recruiter|  Last44|62000.0|\n|        IT|   45|  First45|2015-02-15|      Developer|  Last45|62500.0|\n|   Finance|   46|  First46|2015-02-16|        Analyst|  Last46|63000.0|\n|     Sales|   47|  First47|2015-02-17|Sales Executive|  Last47|63500.0|\n|        HR|   48|  First48|2015-02-18|      Recruiter|  Last48|64000.0|\n|        IT|   49|  First49|2015-02-19|      Developer|  Last49|64500.0|\n|   Finance|   50|  First50|2015-02-20|        Analyst|  Last50|65000.0|\n|        IT|   13|  First13|2015-01-14|      Developer|  Last13|46500.0|\n|   Finance|   14|  First14|2015-01-15|        Analyst|  Last14|47000.0|\n|     Sales|   15|  First15|2015-01-16|Sales Executive|  Last15|47500.0|\n|        HR|   16|  First16|2015-01-17|      Recruiter|  Last16|48000.0|\n|        IT|   17|  First17|2015-01-18|      Developer|  Last17|48500.0|\n|   Finance|   18|  First18|2015-01-19|        Analyst|  Last18|49000.0|\n|     Sales|   19|  First19|2015-01-20|Sales Executive|  Last19|49500.0|\n+----------+-----+---------+----------+---------------+--------+-------+\nonly showing top 20 rows\nroot\n |-- Department: string (nullable = true)\n |-- EmpID: long (nullable = true)\n |-- FirstName: string (nullable = true)\n |-- HireDate: string (nullable = true)\n |-- JobTitle: string (nullable = true)\n |-- LastName: string (nullable = true)\n |-- Salary: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "df_json=spark.read.json(\"/Volumes/workspace/practice/my_volume/employees_json/\")\n",
    "df_json.show()\n",
    "df_json.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2df1fdf2-9a1c-473b-bed5-14dd9f644f2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_json.write.mode(\"overwrite\").saveAsTable(\"workspace.practice.employees_json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e6b5c501-ca21-4b0c-8e96-9dfbadd900e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Multiline JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05b46ab1-0577-47e5-9627-3bc03ebde984",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------+---+-------+------+\n|age|        city| id|   name|salary|\n+---+------------+---+-------+------+\n| 28|    New York|  1|  Alice| 70000|\n| 34| Los Angeles|  2|    Bob| 85000|\n| 25|     Chicago|  3|Charlie| 62000|\n| 45|     Houston|  4|  David| 95000|\n| 30|     Phoenix|  5|    Eva| 72000|\n| 38|Philadelphia|  6|  Frank| 88000|\n| 27| San Antonio|  7|  Grace| 64000|\n| 32|   San Diego|  8| Hannah| 79000|\n| 29|      Dallas|  9|    Ian| 73000|\n| 41|    San Jose| 10|  Julia| 91000|\n+---+------------+---+-------+------+\n\n"
     ]
    }
   ],
   "source": [
    "df_json=spark.read.option(\"multiline\",\"true\") .json(\"/Volumes/workspace/practice/my_volume/json_file.json\")\n",
    "df_json.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de22e054-9141-42f2-884a-5fc7a7e930e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Nested JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0b2c7b9d-4c99-4b4e-ac79-6917fe046417",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------------+-----+--------------------+\n|customer_id|            email| name|              orders|\n+-----------+-----------------+-----+--------------------+\n|          1|alice@example.com|Alice|[{2025-01-10, [{1...|\n|          2|  bob@example.com|  Bob|[{2025-03-15, [{8...|\n+-----------+-----------------+-----+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "df_nested = spark.read.option(\"multiline\",\"true\") .json(\"/Volumes/workspace/practice/my_volume/nested_json.json\")\n",
    "df_nested.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e218b9c-a3d7-4a31-868b-f0a0b25a1240",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- customer_id: integer (nullable = true)\n |-- name: string (nullable = true)\n |-- email: string (nullable = true)\n |-- orders: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- order_id: integer (nullable = true)\n |    |    |-- date: string (nullable = true)\n |    |    |-- items: array (nullable = true)\n |    |    |    |-- element: struct (containsNull = true)\n |    |    |    |    |-- product: string (nullable = true)\n |    |    |    |    |-- quantity: integer (nullable = true)\n |    |    |    |    |-- price: integer (nullable = true)\n\n+-----------+-----+-----------------+--------------------+\n|customer_id| name|            email|              orders|\n+-----------+-----+-----------------+--------------------+\n|          1|Alice|alice@example.com|[{101, 2025-01-10...|\n|          2|  Bob|  bob@example.com|[{201, 2025-03-15...|\n+-----------+-----+-----------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Define schema for nested JSON\n",
    "item_schema = StructType([\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"items\", ArrayType(item_schema), True)\n",
    "])\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"orders\", ArrayType(order_schema), True)\n",
    "])\n",
    "\n",
    "# Read JSON file with schema\n",
    "df = spark.read.option(\"multiLine\", \"true\").schema(customer_schema).json(\"/Volumes/workspace/practice/my_volume/nested_json.json\")\n",
    "\n",
    "# Print schema\n",
    "df.printSchema()\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73bf2a0a-049e-43d5-91c3-01c7082b2d47",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------------+------------------------------------------------------------------------------------------------+\n|customer_id|name |email            |orders                                                                                          |\n+-----------+-----+-----------------+------------------------------------------------------------------------------------------------+\n|1          |Alice|alice@example.com|[{101, 2025-01-10, [{Laptop, 1, 1200}, {Mouse, 2, 25}]}, {102, 2025-02-05, [{Keyboard, 1, 75}]}]|\n|2          |Bob  |bob@example.com  |[{201, 2025-03-15, [{Phone, 1, 800}, {Charger, 1, 20}]}]                                        |\n+-----------+-----+-----------------+------------------------------------------------------------------------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# Show raw nested data\n",
    "df.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "366092db-9eec-4fc3-8ff3-c72477fc556f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------------+--------+----------+\n|customer_id| name|            email|order_id|      date|\n+-----------+-----+-----------------+--------+----------+\n|          1|Alice|alice@example.com|     101|2025-01-10|\n|          1|Alice|alice@example.com|     102|2025-02-05|\n|          2|  Bob|  bob@example.com|     201|2025-03-15|\n+-----------+-----+-----------------+--------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Flatten orders\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "orders_df = df.select(\"customer_id\", \"name\",\"email\", explode(\"orders\").alias(\"order\"))\n",
    "orders_df.select(\"customer_id\", \"name\",\"email\", \"order.order_id\", \"order.date\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0012dfd-4346-4018-bef7-0ce135a7b409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+-----------------+--------+----------+--------+--------+-----+\n|customer_id| name|            email|order_id|      date| product|quantity|price|\n+-----------+-----+-----------------+--------+----------+--------+--------+-----+\n|          1|Alice|alice@example.com|     101|2025-01-10|  Laptop|       1| 1200|\n|          1|Alice|alice@example.com|     101|2025-01-10|   Mouse|       2|   25|\n|          1|Alice|alice@example.com|     102|2025-02-05|Keyboard|       1|   75|\n|          2|  Bob|  bob@example.com|     201|2025-03-15|   Phone|       1|  800|\n|          2|  Bob|  bob@example.com|     201|2025-03-15| Charger|       1|   20|\n+-----------+-----+-----------------+--------+----------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Flatten items inside orders\n",
    "items_df = orders_df.select(\"customer_id\", \"name\",\"email\", \"order.order_id\",\"order.date\", explode(\"order.items\").alias(\"item\"))\n",
    "item_df_final= items_df.select(\"customer_id\", \"name\",\"email\", \"order_id\", \"date\",\"item.product\", \"item.quantity\", \"item.price\")\n",
    "item_df_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "99bd95cb-83ab-4c5b-85cb-eebabb873897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "item_df_final.write.mode(\"overwrite\").saveAsTable(\"workspace.practice.customer_nested_json\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read_JSON",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}